# Overview

This project involves extracting data from MongoDB, cleaning and transforming the data using Python with Pandas and MongoDB commands, and then storing the cleaned data in CSV files. Finally, the CSV files are loaded into a PostgreSQL database.

# Introduction:

In today's data-driven world, the ability to efficiently manage, process, and analyze large volumes of data is crucial for any organization. This project aims to address this need by demonstrating a complete workflow for extracting data from a NoSQL database (MongoDB), cleaning and transforming the data using Python, and then storing the processed data in a relational database (PostgreSQL). This workflow ensures that data can be effectively queried and analyzed using the strengths of both NoSQL and SQL databases.

# Objective: 

The primary objective of this project is to provide a clear and practical guide on how to:

# Extract data from a MongoDB database.

Clean and transform the extracted data using Python's Pandas library and regular expressions.
Save the cleaned data into CSV files.
Load the CSV files into a PostgreSQL database for further analysis.
Execute the above processes in parallel using batch files for improved efficiency.
By following this guide, users will gain a comprehensive understanding of data extraction, transformation, and loading (ETL) processes. They will learn how to leverage the capabilities of different tools and libraries to ensure data integrity and accessibility, thereby enhancing their data processing workflows.

# Scope: 

The project covers the following key areas:

Setting up the necessary environments and dependencies for MongoDB, Python, and PostgreSQL.
Writing Python scripts to connect to MongoDB and extract data.
Performing data cleaning and transformation using Pandas and regular expressions.
Exporting the transformed data to CSV files.
Importing the CSV data into PostgreSQL.
Using batch files to execute these tasks in parallel to optimize performance.
Target Audience: This project is intended for data engineers, data scientists, and software developers who are involved in data processing and analysis. It is also suitable for students and researchers who are looking to understand practical applications of ETL processes using popular data management tools.

By the end of this project, users will have a robust ETL pipeline that efficiently handles data from MongoDB to PostgreSQL, enabling more effective data analysis and decision-making. The incorporation of parallel execution using batch files will further enhance the pipeline's efficiency and scalability.